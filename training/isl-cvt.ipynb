{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11574137,"sourceType":"datasetVersion","datasetId":7256556},{"sourceId":11598263,"sourceType":"datasetVersion","datasetId":7273575}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport csv\nimport pandas as pd\nimport numpy as np\nfrom skimage import io\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot as plt\nimport math\n\n\nimport torch\nfrom torch import nn, Tensor\nfrom torch.nn import functional as F\nfrom torch.utils import data as tdata\nfrom torchvision import utils as vutils, transforms as T\nfrom torchvision.datasets import ImageFolder\n\nfrom transformers import AutoImageProcessor, CvtForImageClassification\nimport pytorch_lightning as pl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T12:56:43.026343Z","iopub.execute_input":"2024-12-09T12:56:43.026805Z","iopub.status.idle":"2024-12-09T12:57:15.631753Z","shell.execute_reply.started":"2024-12-09T12:56:43.026741Z","shell.execute_reply":"2024-12-09T12:57:15.631075Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# defining the number of GPUs because PL makes it very easy for us to\n# parallelize training across multiple GPUs\nnum_devices = torch.cuda.device_count()\nnum_devices","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T12:57:15.63319Z","iopub.execute_input":"2024-12-09T12:57:15.633657Z","iopub.status.idle":"2024-12-09T12:57:15.677393Z","shell.execute_reply.started":"2024-12-09T12:57:15.633631Z","shell.execute_reply":"2024-12-09T12:57:15.676647Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"random_seed = 42\npl.seed_everything(random_seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T12:57:15.678438Z","iopub.execute_input":"2024-12-09T12:57:15.678815Z","iopub.status.idle":"2024-12-09T12:57:15.697072Z","shell.execute_reply.started":"2024-12-09T12:57:15.678769Z","shell.execute_reply":"2024-12-09T12:57:15.696361Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"hugging_model_name = \"microsoft/cvt-13\"\nimage_processor = AutoImageProcessor.from_pretrained(hugging_model_name)\nimage_processor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T12:57:15.69849Z","iopub.execute_input":"2024-12-09T12:57:15.698752Z","iopub.status.idle":"2024-12-09T12:57:16.059347Z","shell.execute_reply.started":"2024-12-09T12:57:15.698706Z","shell.execute_reply":"2024-12-09T12:57:16.058588Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\nfrom sklearn.model_selection import train_test_split\n\n# Define input and output paths\noriginal_data_path = \"/kaggle/input/indian-sign-language-self-creation/isl_dataset\"\naugmented_data_path = \"/kaggle/input/image-augmentation-v3-dataset/augmented_isl_dataset\"\noutput_path = \"/kaggle/train-test-split\"\n\n# Create output directories\ntrain_dir = os.path.join(output_path, \"train\")\nval_dir = os.path.join(output_path, \"val\")\ntest_dir = os.path.join(output_path, \"test\")\nos.makedirs(train_dir, exist_ok=True)\nos.makedirs(val_dir, exist_ok=True)\nos.makedirs(test_dir, exist_ok=True)\n\n# Get class folders from original data\nclasses = [d for d in os.listdir(original_data_path) if os.path.isdir(os.path.join(original_data_path, d))]\n\n# Process original data for train, validation and test sets\nfor class_name in classes:\n    original_class_path = os.path.join(original_data_path, class_name)\n    \n    # Skip if not a directory\n    if not os.path.isdir(original_class_path):\n        continue\n    \n    # Get all images for this class\n    original_images = [os.path.join(original_class_path, img) for img in os.listdir(original_class_path) \n                      if img.lower().endswith(('.png', '.jpg', '.jpeg'))]\n    \n    # First split original images into temp_train and test (80/20)\n    temp_train_images, test_images = train_test_split(original_images, test_size=0.2, random_state=42)\n    \n    # Further split temp_train into actual train and validation (75/25 of temp_train, which is 60/20 of total)\n    train_images, val_images = train_test_split(temp_train_images, test_size=0.25, random_state=42)\n    \n    # Create class directories in train, val, and test\n    os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)\n    os.makedirs(os.path.join(val_dir, class_name), exist_ok=True)\n    os.makedirs(os.path.join(test_dir, class_name), exist_ok=True)\n    \n    # Copy original images to train, val, and test directories\n    for img in train_images:\n        shutil.copy(img, os.path.join(train_dir, class_name))\n    \n    for img in val_images:\n        shutil.copy(img, os.path.join(val_dir, class_name))\n    \n    for img in test_images:\n        shutil.copy(img, os.path.join(test_dir, class_name))\n    \n    # Add augmented data to training set only\n    augmented_class_path = os.path.join(augmented_data_path, class_name)\n    if os.path.exists(augmented_class_path) and os.path.isdir(augmented_class_path):\n        augmented_images = [os.path.join(augmented_class_path, img) for img in os.listdir(augmented_class_path)\n                           if img.lower().endswith(('.png', '.jpg', '.jpeg'))]\n        \n        # Copy all augmented images to train directory only\n        for img in augmented_images:\n            shutil.copy(img, os.path.join(train_dir, class_name))\n\nprint(f\"Data split complete!\")\nprint(f\"Training data (original + augmented) is in '{train_dir}'\")\nprint(f\"Validation data (original only) is in '{val_dir}'\")\nprint(f\"Testing data (original only) is in '{test_dir}'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T12:57:16.060297Z","iopub.execute_input":"2024-12-09T12:57:16.060543Z","iopub.status.idle":"2024-12-09T12:57:21.420935Z","shell.execute_reply.started":"2024-12-09T12:57:16.060511Z","shell.execute_reply":"2024-12-09T12:57:21.42003Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"kaggle_write_path = '/kaggle/working/'\nmean, std = image_processor.image_mean, image_processor.image_std\nimg_size = image_processor.size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T12:57:21.421881Z","iopub.execute_input":"2024-12-09T12:57:21.422121Z","iopub.status.idle":"2024-12-09T12:57:21.426688Z","shell.execute_reply.started":"2024-12-09T12:57:21.422097Z","shell.execute_reply":"2024-12-09T12:57:21.425921Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_dirs(root_path):\n    \"\"\"\n        Returns `(list_classes, classes_to_idx, numpy array mapping path to class_idx)\n    \"\"\"\n    if not os.path.exists(root_path):\n        raise FileNotFoundError(\"Folder does not exist\")\n        \n    classes = sorted(os.listdir(root_path))\n    classes_to_idx = {c: i for i, c in enumerate(classes)}\n    \n    all_samples = []\n    \n    for idx, cl in enumerate(classes):\n        path = os.path.join(root_path, cl)\n        \n        all_files = os.listdir(path)\n        \n        # add `(path, class_idx)` to all_samples\n        all_samples.extend([[p, idx] for p in all_files])\n\n    all_samples = np.array(all_samples)\n    \n    return classes, classes_to_idx, all_samples\n    \n# all_classes, classes_to_idx, all_samples = get_dirs(train_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T12:57:21.427592Z","iopub.execute_input":"2024-12-09T12:57:21.427843Z","iopub.status.idle":"2024-12-09T12:57:21.507509Z","shell.execute_reply.started":"2024-12-09T12:57:21.427819Z","shell.execute_reply":"2024-12-09T12:57:21.506467Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(f\"Number of samples: {len(all_samples)}\")\n# train_ratio = 0.8\n# train_samples, validation_samples = train_test_split(all_samples, shuffle=True, train_size=train_ratio, stratify=all_samples[:, 1], random_state=42)\n# print(f\"Length of splits: {len(train_samples)}, {len(validation_samples)}\")\n# train_samples[0], validation_samples[0]\n\nall_classes, classes_to_idx, train_samples = get_dirs(train_dir)\n_, _, validation_samples = get_dirs(val_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"classes_to_idx","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport random\nimport matplotlib.pyplot as plt\nfrom skimage import io\n\n\ndef visualize_dataset_examples(dataset_path, classes, num_cols=5, title=\"Dataset Examples\"):\n    \"\"\"\n    Display one random example from each class in the dataset\n    \n    Args:\n        dataset_path: Path to the dataset (train_dir, val_dir)\n        classes: List of class names\n        num_cols: Number of columns in the grid\n        title: Title for the plot\n    \"\"\"\n    # Create a figure\n    num_classes = len(classes)\n    num_rows = (num_classes + num_cols - 1) // num_cols\n    \n    plt.figure(figsize=(3*num_cols, 3*num_rows))\n    plt.suptitle(title, fontsize=16)\n    \n    # For each class, display one random image\n    for i, class_name in enumerate(classes):\n        class_path = os.path.join(dataset_path, class_name)\n        \n        # Skip if the class directory doesn't exist\n        if not os.path.exists(class_path):\n            continue\n            \n        # Get all image files\n        images = [img for img in os.listdir(class_path) \n                 if img.lower().endswith(('.png', '.jpg', '.jpeg'))]\n        \n        if images:\n            # Select a random image\n            random_img = random.choice(images)\n            img_path = os.path.join(class_path, random_img)\n            \n            # Read and display the image\n            img = io.imread(img_path)\n            \n            plt.subplot(num_rows, num_cols, i+1)\n            plt.imshow(img)\n            plt.title(class_name, fontsize=10)\n            plt.axis('off')\n    \n    plt.tight_layout(rect=[0, 0, 1, 0.97])\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Visualizing training set examples (one per class)...\")\nvisualize_dataset_examples(train_dir, all_classes, title=\"Training Set Examples\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Visualizing validation set examples (one per class)...\")\nvisualize_dataset_examples(val_dir, all_classes, title=\"Validation Set Examples\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CustomDataset(tdata.Dataset):\n    def __init__(self, root_path, samples, classes, transform=None, transform_args=[]):\n        \"\"\"_summary_\n\n        Args:\n            root_path (_type_): path from where data will be read. Like `ImageFolder`, pass in folder path where folders inside it will be treated as classes.\n            samples (_type_): a numpy array that will have rows that map `(image_path, class_idx)`\n            classes (_type_): a list of all class names\n            transform (_type_, optional): If provided, can be torch transforms or 🤗 image processors. Defaults to None.\n            transform_args (list, optional): Is a list of `(argument, value)`. Defaults to [].\n        \"\"\"\n        \n        self.root_path = root_path\n        self.samples = samples\n        self.transform = transform\n        self.classes = classes\n        print(classes)\n        self.transform_args = {}\n        for key, val in transform_args:\n            self.transform_args[key] = val\n        \n        print(self.transform_args)\n        \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        file_name, target = self.samples[idx]\n        target = int(target)\n        file_path = os.path.join(self.root_path, self.classes[target], file_name)\n        img = io.imread(file_path)\n        \n        if self.transform is not None:\n            img = self.transform(img, **self.transform_args)\n        \n        return img[\"pixel_values\"][0], target","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"idx_to_class = {i: d for (i, d) in enumerate(all_classes)}\nnum_classes = len(all_classes)\nstr([num_classes, idx_to_class])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T12:57:21.551999Z","iopub.execute_input":"2024-12-09T12:57:21.552764Z","iopub.status.idle":"2024-12-09T12:57:21.568219Z","shell.execute_reply.started":"2024-12-09T12:57:21.552706Z","shell.execute_reply":"2024-12-09T12:57:21.567434Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transform_args = [(\"return_tensors\", 'pt')]\n\ntrain_dataset = CustomDataset(\n    train_dir, train_samples, all_classes, transform=image_processor, transform_args=transform_args)\nvalid_dataset = CustomDataset(val_dir, validation_samples, all_classes,\n                              transform=image_processor, transform_args=transform_args)\nlen(train_dataset), len(valid_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T12:57:21.56911Z","iopub.execute_input":"2024-12-09T12:57:21.569346Z","iopub.status.idle":"2024-12-09T12:57:21.581968Z","shell.execute_reply.started":"2024-12-09T12:57:21.569322Z","shell.execute_reply":"2024-12-09T12:57:21.58121Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset[0][0].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T12:57:21.582972Z","iopub.execute_input":"2024-12-09T12:57:21.583285Z","iopub.status.idle":"2024-12-09T12:57:21.648884Z","shell.execute_reply.started":"2024-12-09T12:57:21.583249Z","shell.execute_reply":"2024-12-09T12:57:21.648104Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def denorm(tensor, mean=mean, std=std):\n  \"\"\"This denormalises the tensor so that we can use it to plot normalised images\"\"\"\n  mean, std = torch.tensor([mean]), torch.tensor([std])\n  # print(mean.shape)\n  output = std * tensor + mean\n  return torch.clamp(output, 0, 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T12:57:21.64989Z","iopub.execute_input":"2024-12-09T12:57:21.650459Z","iopub.status.idle":"2024-12-09T12:57:21.654627Z","shell.execute_reply.started":"2024-12-09T12:57:21.650421Z","shell.execute_reply":"2024-12-09T12:57:21.653932Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_image(tensor, label=None, denormalise=True):\n  \"\"\"tensor is of shape `(h, w, c)`\"\"\"\n  plt.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n\n  if label is not None:\n    plt.title(str(label))\n  \n  if denormalise:\n    tensor = denorm(tensor)\n    tensor.permute(1, 2, 0)\n    \n  plt.imshow(tensor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T12:57:21.655604Z","iopub.execute_input":"2024-12-09T12:57:21.655851Z","iopub.status.idle":"2024-12-09T12:57:21.671178Z","shell.execute_reply.started":"2024-12-09T12:57:21.655827Z","shell.execute_reply":"2024-12-09T12:57:21.670567Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_grid(tensor, n_row):\n  \"\"\"Input is of shape (batch_size, c, h, w)\"\"\"\n\n  grid_tensor = vutils.make_grid(tensor, n_row).permute(1, 2, 0)\n  plot_image(grid_tensor, \"Grid of Random ISL Images\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T12:57:21.672078Z","iopub.execute_input":"2024-12-09T12:57:21.67228Z","iopub.status.idle":"2024-12-09T12:57:21.681666Z","shell.execute_reply.started":"2024-12-09T12:57:21.672259Z","shell.execute_reply":"2024-12-09T12:57:21.681032Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img = train_dataset[1][0].permute(1, 2, 0)\nplot_image(img)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T12:57:21.68263Z","iopub.execute_input":"2024-12-09T12:57:21.682975Z","iopub.status.idle":"2024-12-09T12:57:22.102881Z","shell.execute_reply.started":"2024-12-09T12:57:21.68294Z","shell.execute_reply":"2024-12-09T12:57:22.102061Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import BatchSampler, SequentialSampler\n\nbatch_size = 64\n\ndef get_loader(dataset, batch_size, num_workers=0):\n    \"\"\"Simple Utility function to instantiate a `DataLoader`.\"\"\"\n    \n    persistent_workers = True if num_workers > 0 else False\n    \n    sampler = BatchSampler(SequentialSampler(dataset), batch_size, drop_last=False)\n    return tdata.DataLoader(dataset, batch_sampler=sampler, \n                            num_workers=num_workers, \n                            # the below flag is necessary for it to not crash\n                            persistent_workers=persistent_workers)\n\ntrain_loader = get_loader(train_dataset, batch_size, 2)\nvalid_loader = get_loader(valid_dataset, batch_size, 2)\n\n# check if the dataloader works fine by plotting a grid of images\nfor images, labels in train_loader:\n    print(images.shape)\n    plot_grid(images, 8)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T12:57:22.103975Z","iopub.execute_input":"2024-12-09T12:57:22.104241Z","iopub.status.idle":"2024-12-09T12:57:23.809092Z","shell.execute_reply.started":"2024-12-09T12:57:22.104214Z","shell.execute_reply":"2024-12-09T12:57:23.807979Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CvTTrainingModule(pl.LightningModule):\n\n  def __init__(self, num_labels: int, hugging_model_name: str, lr: int = 1e-6):\n    super(CvTTrainingModule, self).__init__()\n    self.cvt = CvtForImageClassification.from_pretrained(hugging_model_name, num_labels=num_labels,\n                                                         id2label=idx_to_class, label2id=classes_to_idx,\n                                                         ignore_mismatched_sizes=True)\n    self.lr = lr\n    # Freezing early layers initially\n    for param in self.cvt.base_model.parameters():\n        param.requires_grad = False\n\n  def forward(self, pixel_values):\n    outputs = self.cvt(pixel_values)\n    return outputs[\"logits\"]\n\n  def common_step(self, batch):\n    # images, labels = batch\n    # outputs = self.cvt(images)[\"logits\"]\n\n    # l = F.cross_entropy(outputs, labels)\n    # preds = torch.argmax(outputs, dim=-1)\n    # num_correct = (preds == labels).sum().item()\n    # accuracy = num_correct / len(batch)\n\n    # return l, accuracy\n    images, labels = batch\n    outputs = self.cvt(images)[\"logits\"]\n\n    l = F.cross_entropy(outputs, labels)\n    _, preds = torch.max(outputs, dim=1)\n    num_correct = (preds == labels).sum().item()\n    accuracy = num_correct / labels.size(0)  # Correction here to divide by the size of labels in the batch\n    \n    return l, accuracy\n\n  def training_step(self, batch, batch_idx):\n\n    l, accuracy = self.common_step(batch)\n\n    self.log(\"training_loss\", l)\n    self.log(\"training_accuracy\", accuracy)\n\n    return l\n\n  def validation_step(self, batch, batch_idx):\n    l, accuracy = self.common_step(batch)\n\n    self.log(\"val_loss\", l)\n    self.log(\"val_accuracy\", accuracy)\n\n    return l\n\n  # def configure_optimizers(self):\n  #   optimizer = torch.optim.AdamW(self.parameters(), self.lr)\n  #   return optimizer\n  def configure_optimizers(self):\n    optimizer = torch.optim.AdamW(self.parameters(), self.lr)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', factor=0.1, patience=5, min_lr=1e-9, verbose=True\n    )\n    return {'optimizer': optimizer, 'lr_scheduler': scheduler, 'monitor': 'val_loss'}\n      \n  def unfreeze_model(self):\n    for param in self.cvt.base_model.parameters():\n        param.requires_grad = True\n\n  def train_dataloader(self):\n    return train_loader\n\n  def val_dataloader(self):\n    return valid_loader\n      \n  def save_model(self, save_path: str):\n    self.cvt.save_pretrained(save_path, from_pt=True)\n\n\n# class CvTTrainingModule(pl.LightningModule):\n\n#   def __init__(self, num_labels: int, hugging_model_name: str, lr: float = 5e-5, \n#                weight_decay: float = 1e-2, dropout_rate: float = 0.3, \n#                label_smoothing: float = 0.1, use_scheduler: bool = True):\n#     super(CvTTrainingModule, self).__init__()\n#     self.cvt = CvtForImageClassification.from_pretrained(hugging_model_name, num_labels=num_labels,\n#                                                          id2label=idx_to_class, label2id=classes_to_idx,\n#                                                          ignore_mismatched_sizes=True)\n    \n#     # CvT uses embed_dim, not hidden_size\n#     # The last element in embed_dim list is the final dimension\n#     feature_dim = self.cvt.config.embed_dim[-1]\n    \n  #   # Add dropout to classification head\n  #   self.dropout = nn.Dropout(dropout_rate)\n  #   self.classifier = nn.Linear(feature_dim, num_labels)\n  #   self.cvt.classifier = nn.Identity()  # Remove the original classifier\n    \n  #   # Save hyperparameters\n  #   self.lr = lr\n  #   self.weight_decay = weight_decay\n  #   self.label_smoothing = label_smoothing\n  #   self.use_scheduler = use_scheduler\n  #   self.save_hyperparameters(ignore=['cvt'])\n\n  # def forward(self, pixel_values):\n  #   features = self.cvt(pixel_values).logits  # Get features from the backbone\n  #   features = self.dropout(features)  # Apply dropout\n  #   logits = self.classifier(features)  # Apply classification head\n  #   return logits\n\n  # def common_step(self, batch):\n  #   images, labels = batch\n    \n  #   # Get logits\n  #   logits = self(images)\n    \n  #   # Apply label smoothing to loss\n  #   l = F.cross_entropy(logits, labels, label_smoothing=self.label_smoothing)\n    \n  #   # Calculate accuracy\n  #   _, preds = torch.max(logits, dim=1)\n  #   num_correct = (preds == labels).sum().item()\n  #   accuracy = num_correct / labels.size(0)\n    \n  #   return l, accuracy\n\n  # def training_step(self, batch, batch_idx):\n  #   l, accuracy = self.common_step(batch)\n    \n  #   # Logging\n  #   self.log(\"training_loss\", l)\n  #   self.log(\"training_accuracy\", accuracy)\n    \n  #   # Log learning rate\n  #   opt = self.optimizers()\n  #   if opt is not None:\n  #       self.log('learning_rate', opt.param_groups[0]['lr'])\n    \n  #   return l\n\n  # # In your CvTTrainingModule class:\n  # def validation_step(self, batch, batch_idx):\n  #   images, labels = batch\n  #   outputs = self.cvt(images)[\"logits\"]\n    \n  #   l = F.cross_entropy(outputs, labels, label_smoothing=self.label_smoothing)\n  #   _, preds = torch.max(outputs, dim=1)\n  #   num_correct = (preds == labels).sum().item()\n  #   accuracy = num_correct / labels.size(0)\n    \n  #   # THIS IS THE KEY FIX - proper metric logging\n  #   self.log(\"val_loss\", l, on_step=False, on_epoch=True, prog_bar=True)\n  #   self.log(\"val_accuracy\", accuracy, on_step=False, on_epoch=True, prog_bar=True)\n    \n  #   return l\n\n  # def configure_optimizers(self):\n  #   # Add weight decay for regularization\n  #   optimizer = torch.optim.AdamW(\n  #       self.parameters(), \n  #       lr=self.lr,\n  #       weight_decay=self.weight_decay\n  #   )\n    \n  #   if not self.use_scheduler:\n  #       return optimizer\n    \n  #   # Learning rate scheduler with warmup\n  #   train_steps = len(train_loader) * self.trainer.max_epochs\n  #   warmup_steps = int(0.1 * train_steps)  # 10% warmup\n    \n  #   scheduler = {\n  #       'scheduler': torch.optim.lr_scheduler.OneCycleLR(\n  #           optimizer,\n  #           max_lr=self.lr,\n  #           total_steps=train_steps,\n  #           pct_start=0.1,  # Warmup percentage\n  #           div_factor=25,  # Initial lr = max_lr/div_factor\n  #           final_div_factor=1000,  # Final lr = initial_lr/final_div_factor\n  #       ),\n  #       'interval': 'step',\n  #       'frequency': 1\n  #   }\n    \n  #   return {'optimizer': optimizer, 'lr_scheduler': scheduler}\n\n  # def on_before_optimizer_step(self, optimizer):\n  #   # Gradient clipping to prevent explosion\n  #   torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)\n    \n  # def train_dataloader(self):\n  #   return train_loader\n\n  # def val_dataloader(self):\n  #   return valid_loader\n      \n  # def save_model(self, save_path: str):\n  #   # Create directory if it doesn't exist\n  #   os.makedirs(save_path, exist_ok=True)\n  #   # Save the entire model\n  #   torch.save(self.state_dict(), os.path.join(save_path, \"model.pt\"))\n  #   # Save the backbone for HF compatibility\n  #   self.cvt.save_pretrained(save_path, from_pt=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class UnfreezeCallback(pl.Callback):\n    def __init__(self, unfreeze_at_epoch: int):\n        super().__init__()\n        self.unfreeze_at_epoch = unfreeze_at_epoch\n\n    def on_train_epoch_end(self, trainer, pl_module):\n        if trainer.current_epoch == self.unfreeze_at_epoch:\n            pl_module.unfreeze_model()\n            print(f\"Unfreezing model at epoch {trainer.current_epoch}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport os\nimport glob\nfrom pytorch_lightning.callbacks import Callback\n\nclass LearningCurveCallback(Callback):\n    def __init__(self):\n        super().__init__()\n        self.train_losses = []\n        self.val_losses = []\n        self.train_accuracies = []\n        self.val_accuracies = []\n        \n    def on_train_epoch_end(self, trainer, pl_module):\n        # Store metrics\n        metrics = trainer.callback_metrics\n        self.train_losses.append(metrics.get('training_loss').item())\n        self.val_losses.append(metrics.get('val_loss').item())\n        self.train_accuracies.append(metrics.get('training_accuracy').item())\n        self.val_accuracies.append(metrics.get('val_accuracy').item())\n        \n    def plot_learning_curves(self):\n        epochs = range(1, len(self.train_losses) + 1)\n        \n        # Create figure with two subplots\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n        \n        # Plot losses\n        ax1.plot(epochs, self.train_losses, 'b-', label='Training Loss')\n        ax1.plot(epochs, self.val_losses, 'r-', label='Validation Loss')\n        ax1.set_title('Loss Learning Curves')\n        ax1.set_xlabel('Epochs')\n        ax1.set_ylabel('Loss')\n        ax1.legend()\n        ax1.grid(True, linestyle='--', alpha=0.7)\n        \n        # Plot accuracies\n        ax2.plot(epochs, self.train_accuracies, 'b-', label='Training Accuracy')\n        ax2.plot(epochs, self.val_accuracies, 'r-', label='Validation Accuracy')\n        ax2.set_title('Accuracy Learning Curves')\n        ax2.set_xlabel('Epochs')\n        ax2.set_ylabel('Accuracy')\n        ax2.legend()\n        ax2.grid(True, linestyle='--', alpha=0.7)\n        \n        plt.tight_layout()\n        plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%load_ext tensorboard\n%tensorboard --logdir /kaggle/working/lightning_logs/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T12:57:23.821349Z","iopub.execute_input":"2024-12-09T12:57:23.821685Z","iopub.status.idle":"2024-12-09T12:57:30.375974Z","shell.execute_reply.started":"2024-12-09T12:57:23.821649Z","shell.execute_reply":"2024-12-09T12:57:30.374892Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define paths\ncustom_checkpoint_path = os.path.join(kaggle_write_path, 'custom.pth')\ncheckpoint_path = os.path.join(kaggle_write_path, \"checkpoints\")\n\nlearning_curve_callback = LearningCurveCallback()\n\n# Define callbacks\ncheckpoint_callback = pl.callbacks.ModelCheckpoint(\n    dirpath=checkpoint_path,\n    save_top_k=2,  # Save top 2 models with the lowest validation loss\n    every_n_train_steps=50,  # Save every 50 steps\n    mode=\"min\",  # Minimize the monitored metric\n    monitor=\"val_loss\",  # Monitor validation loss\n    filename=\"{epoch:02d}-{val_loss:.2f}\",  # Filename format\n    verbose=True  # Log saving checkpoints\n)\n\nearly_stop_callback = pl.callbacks.EarlyStopping(\n    monitor='val_loss',  # Monitor validation loss\n    patience=15,          # Stop after 5 epochs with no improvement\n    verbose=True,        # Log when stopping\n    mode='min'           # Minimize validation loss\n)\n\nunfreeze_callback = UnfreezeCallback(unfreeze_at_epoch=5) \n\n# Define model\n# num_classes = 35  # Replace with your number of classes\nnum_classes = 77 # Replace with your number of classes\nhugging_model_name = \"microsoft/cvt-13\"\n\nmodel = CvTTrainingModule(num_classes, hugging_model_name)\n\n# Trainer arguments\nnum_devices = 1  # Set the number of GPUs\ntrainer_args = {\n    \"accelerator\": \"gpu\",\n    \"devices\": num_devices,\n    \"strategy\": \"auto\",\n    \"log_every_n_steps\": 5,\n    \"callbacks\": [early_stop_callback, checkpoint_callback, learning_curve_callback, unfreeze_callback],\n    \"max_epochs\": 100,\n    \"check_val_every_n_epoch\": 1,  # Force validation every epoch\n    \"num_sanity_val_steps\": 2,     # Run validation sanity checks\n}\n\n# Initialize Trainer\ntrainer = pl.Trainer(**trainer_args)\n\n# Start training\n# trainer.fit(model, ckpt_path=None)\ntrainer.fit(model, ckpt_path=None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T12:57:30.377144Z","iopub.execute_input":"2024-12-09T12:57:30.377437Z","iopub.status.idle":"2024-12-09T13:01:55.603357Z","shell.execute_reply.started":"2024-12-09T12:57:30.377409Z","shell.execute_reply":"2024-12-09T13:01:55.602607Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"learning_curve_callback.plot_learning_curves()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_model('/kaggle/working/cvt_model') ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T23:28:03.982218Z","iopub.execute_input":"2025-03-12T23:28:03.982460Z","iopub.status.idle":"2025-03-12T23:28:04.224854Z","shell.execute_reply.started":"2025-03-12T23:28:03.982403Z","shell.execute_reply":"2025-03-12T23:28:04.223586Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# Define the mapping for class names (1-9 and A-Z)\n# classes_to_idx = {str(i): i - 1 for i in range(1, 10)}  # Map '1'-'9' to indices 0-8\n# classes_to_idx.update({chr(i): i - ord('A') + 9 for i in range(ord('A'), ord('Z') + 1)})  # Map 'A'-'Z' to indices 9-34\n\n# Define the mapping for class names (A-Z)\n# classes_to_idx= {chr(i): i - ord('A') for i in range(ord('A'), ord('Z') + 1)}\ntest_path = '/kaggle/train-test-split/test'\n\nsamples = []\nvisual_samples = []\n# Iterate through subfolders\nfor class_name in sorted(os.listdir(test_path)):\n    class_folder = os.path.join(test_path, class_name)\n    if os.path.isdir(class_folder):  # Ensure it's a directory\n        idx = classes_to_idx[class_name]  # Map class name to index\n        vis_taken = 0\n        # Iterate through images in the subfolder\n        for f in sorted(os.listdir(class_folder)):\n            p = os.path.join(class_folder, f)\n            if vis_taken == 0:\n                visual_samples.append((p, idx))\n                vis_taken = 1\n            samples.append((p, idx))\n\nprint(f\"Total samples: {len(samples)}\")\nprint(f\"Visual samples: {len(visual_samples)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T13:01:55.604948Z","iopub.execute_input":"2024-12-09T13:01:55.605815Z","iopub.status.idle":"2024-12-09T13:01:55.613866Z","shell.execute_reply.started":"2024-12-09T13:01:55.605765Z","shell.execute_reply":"2024-12-09T13:01:55.613111Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def perform_predictions(samples, model):\n    \"\"\"samples will be of the form `(path, target)`\"\"\"\n    num_correct = 0\n    \n    fig, ax = plt.subplots(len(samples), sharey=True, figsize=(5, 50))\n    model.eval()\n    for curr_ax, (p, idx) in zip(ax, samples):\n        img = io.imread(p)\n        proc_img = image_processor(img, return_tensors=\"pt\")[\"pixel_values\"]\n        outputs = model(proc_img)\n        \n        pred = torch.argmax(outputs, -1)\n        if pred == idx:\n            num_correct += 1\n        \n        target_class = all_classes[idx]\n        curr_ax.title.set_text(f\"Actual class: {target_class}, Predicted: {all_classes[pred]}\")\n        curr_ax.tick_params(bottom=False, left=False, labelleft=False,\n                            labelright=False, labelbottom=False)\n        curr_ax.imshow(img)\n        \n    plt.show()\n    return num_correct\n    \nperform_predictions(visual_samples, model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T13:01:55.614798Z","iopub.execute_input":"2024-12-09T13:01:55.615198Z","iopub.status.idle":"2024-12-09T13:02:02.010643Z","shell.execute_reply.started":"2024-12-09T13:01:55.615152Z","shell.execute_reply":"2024-12-09T13:02:02.009692Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,  confusion_matrix\nimport torch\nimport matplotlib.pyplot as plt\nimport skimage.io as io\nfrom tqdm import tqdm  # Optional, for progress display\nfrom PIL import Image\nimport seaborn as sns\n\nall_preds = []\nall_labels = []\n\ndef evaluate_model(samples, model, image_processor):\n    \"\"\"samples will be of the form `(path, target)`\"\"\"\n    fig, ax = plt.subplots(len(samples), sharey=True, figsize=(5, 50))\n    model.eval()\n    for curr_ax, (p, idx) in zip(ax, samples):\n        img = io.imread(p)\n        proc_img = image_processor(img, return_tensors=\"pt\")[\"pixel_values\"]\n        outputs = model(proc_img)\n        \n        pred = torch.argmax(outputs, -1)\n        all_preds.append(pred)\n        all_labels.append(idx)\n\n    # Calculate accuracy, precision, recall, and F1 score\n    accuracy = accuracy_score(all_labels, all_preds)\n    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n    \n    # Print the results\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1 Score: {f1:.4f}\")\n    \n    return accuracy, precision, recall, f1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T13:02:02.01183Z","iopub.execute_input":"2024-12-09T13:02:02.012121Z","iopub.status.idle":"2024-12-09T13:02:02.019464Z","shell.execute_reply.started":"2024-12-09T13:02:02.012092Z","shell.execute_reply":"2024-12-09T13:02:02.018639Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"evaluate_model(samples, model, image_processor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T13:02:02.020495Z","iopub.execute_input":"2024-12-09T13:02:02.020776Z","iopub.status.idle":"2024-12-09T13:02:18.835971Z","shell.execute_reply.started":"2024-12-09T13:02:02.020719Z","shell.execute_reply":"2024-12-09T13:02:18.834997Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cm = confusion_matrix(all_labels, all_preds)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[idx_to_class[i] for i in set(all_labels)], yticklabels=[idx_to_class[i] for i in set(all_labels)])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T16:49:04.578892Z","iopub.status.idle":"2025-03-09T16:49:04.579348Z","shell.execute_reply.started":"2025-03-09T16:49:04.579104Z","shell.execute_reply":"2025-03-09T16:49:04.579131Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torchview\nimport random\nfrom transformers import AutoModel\nfrom torchview import draw_graph\n\nrandom_sample = random.choice(samples)\nimage_path_random, label = random_sample\n\nrandom_image = io.imread(image_path_random)\nproc_rand_img = image_processor(random_image, return_tensors=\"pt\")[\"pixel_values\"]\noutputs = model(proc_rand_img)\n\nmodel_graph = draw_graph(model, input_data=proc_rand_img)\n\nmodel_graph.visual_graph","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T16:49:04.580853Z","iopub.status.idle":"2025-03-09T16:49:04.581352Z","shell.execute_reply.started":"2025-03-09T16:49:04.581093Z","shell.execute_reply":"2025-03-09T16:49:04.581121Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}