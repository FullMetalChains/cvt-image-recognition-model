{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10141653,"sourceType":"datasetVersion","datasetId":6259601}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport csv\nimport pandas as pd\nimport numpy as np\nfrom skimage import io\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot as plt\nimport math\n\n\nimport torch\nfrom torch import nn, Tensor\nfrom torch.nn import functional as F\nfrom torch.utils import data as tdata\nfrom torchvision import utils as vutils, transforms as T\nfrom torchvision.datasets import ImageFolder\n\nfrom transformers import AutoImageProcessor, ResNetForImageClassification\nimport pytorch_lightning as pl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T13:49:11.263257Z","iopub.execute_input":"2024-12-09T13:49:11.263625Z","iopub.status.idle":"2024-12-09T13:49:11.269385Z","shell.execute_reply.started":"2024-12-09T13:49:11.263584Z","shell.execute_reply":"2024-12-09T13:49:11.268356Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# defining the number of GPUs because PL makes it very easy for us to\n# parallelize training across multiple GPUs\nnum_devices = torch.cuda.device_count()\nnum_devices","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T13:49:11.270862Z","iopub.execute_input":"2024-12-09T13:49:11.271165Z","iopub.status.idle":"2024-12-09T13:49:11.289759Z","shell.execute_reply.started":"2024-12-09T13:49:11.271141Z","shell.execute_reply":"2024-12-09T13:49:11.289071Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"random_seed = 42\npl.seed_everything(random_seed)\n\n# as we can see the above sets the seed for everything. this produces 51 each time\nnp.random.randint(0, 100)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T13:49:11.290743Z","iopub.execute_input":"2024-12-09T13:49:11.291200Z","iopub.status.idle":"2024-12-09T13:49:11.306256Z","shell.execute_reply.started":"2024-12-09T13:49:11.291174Z","shell.execute_reply":"2024-12-09T13:49:11.305388Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"hugging_model_name = \"microsoft/resnet-50\"\nimage_processor = AutoImageProcessor.from_pretrained(hugging_model_name)\nimage_processor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T13:49:11.307751Z","iopub.execute_input":"2024-12-09T13:49:11.308046Z","iopub.status.idle":"2024-12-09T13:49:11.362571Z","shell.execute_reply.started":"2024-12-09T13:49:11.308008Z","shell.execute_reply":"2024-12-09T13:49:11.361821Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\nfrom sklearn.model_selection import train_test_split\n\n# Define input and output paths\ninput_path = \"/kaggle/input/renamed-indian-sign-language-dataset/indian-sign-language-dataset/ISL_Dataset\"\noutput_path = \"/kaggle/train-test-split\"\n\ntrain_dir = os.path.join(output_path, \"train\")\ntest_dir = os.path.join(output_path, \"test\")\n\n# Create output directories\nos.makedirs(train_dir, exist_ok=True)\nos.makedirs(test_dir, exist_ok=True)\n\n# Get all class folders\nclasses = [d for d in os.listdir(input_path) if os.path.isdir(os.path.join(input_path, d))]\n\n# Train-test split ratio\ntest_size = 0.2\n\nfor class_name in classes:\n    class_path = os.path.join(input_path, class_name)\n    images = [os.path.join(class_path, img) for img in os.listdir(class_path) if img.endswith(('.png', '.jpg', '.jpeg'))]\n\n    # Split images into training and testing\n    train_images, test_images = train_test_split(images, test_size=test_size, random_state=42)\n\n    # Create class directories in train and test\n    os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)\n    os.makedirs(os.path.join(test_dir, class_name), exist_ok=True)\n\n    # Move images to the corresponding directories\n    for img in train_images:\n        shutil.copy(img, os.path.join(train_dir, class_name))\n    for img in test_images:\n        shutil.copy(img, os.path.join(test_dir, class_name))\n\nprint(f\"Data split complete! Training data is in '{train_dir}', and testing data is in '{test_dir}'.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T13:49:11.363443Z","iopub.execute_input":"2024-12-09T13:49:11.363681Z","iopub.status.idle":"2024-12-09T13:49:12.139923Z","shell.execute_reply.started":"2024-12-09T13:49:11.363658Z","shell.execute_reply":"2024-12-09T13:49:12.139025Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_path = '/kaggle/train-test-split/train'\nkaggle_write_path = '/kaggle/working/'\nmean, std = image_processor.image_mean, image_processor.image_std\nimg_size = image_processor.size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T13:49:12.140805Z","iopub.execute_input":"2024-12-09T13:49:12.141110Z","iopub.status.idle":"2024-12-09T13:49:12.145830Z","shell.execute_reply.started":"2024-12-09T13:49:12.141084Z","shell.execute_reply":"2024-12-09T13:49:12.144817Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_dirs(root_path):\n    \"\"\"\n        Returns `(list_classes, classes_to_idx, numpy array mapping path to class_idx)\n    \"\"\"\n    if not os.path.exists(root_path):\n        raise FileNotFoundError(\"Folder does not exist\")\n        \n    classes = sorted(os.listdir(root_path))\n    classes_to_idx = {c: i for i, c in enumerate(classes)}\n    \n    all_samples = []\n    \n    for idx, cl in enumerate(classes):\n        path = os.path.join(root_path, cl)\n        \n        all_files = os.listdir(path)\n        \n        # add `(path, class_idx)` to all_samples\n        all_samples.extend([[p, idx] for p in all_files])\n\n    all_samples = np.array(all_samples)\n    \n    return classes, classes_to_idx, all_samples\n    \nall_classes, classes_to_idx, all_samples = get_dirs(train_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T13:49:12.147118Z","iopub.execute_input":"2024-12-09T13:49:12.147800Z","iopub.status.idle":"2024-12-09T13:49:12.168878Z","shell.execute_reply.started":"2024-12-09T13:49:12.147773Z","shell.execute_reply":"2024-12-09T13:49:12.168216Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Number of samples: {len(all_samples)}\")\ntrain_ratio = 0.9\ntrain_samples, test_samples = train_test_split(all_samples, shuffle=True, train_size=train_ratio)\nprint(f\"Length of splits: {len(train_samples)}, {len(test_samples)}\")\ntrain_samples[0], test_samples[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T13:49:12.169879Z","iopub.execute_input":"2024-12-09T13:49:12.170195Z","iopub.status.idle":"2024-12-09T13:49:12.179620Z","shell.execute_reply.started":"2024-12-09T13:49:12.170161Z","shell.execute_reply":"2024-12-09T13:49:12.178811Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"idx_to_class = {i: d for (i, d) in enumerate(all_classes)}\nnum_classes = len(all_classes)\nstr([num_classes, idx_to_class])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T13:49:12.182427Z","iopub.execute_input":"2024-12-09T13:49:12.182688Z","iopub.status.idle":"2024-12-09T13:49:12.194631Z","shell.execute_reply.started":"2024-12-09T13:49:12.182664Z","shell.execute_reply":"2024-12-09T13:49:12.193824Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CustomDataset(tdata.Dataset):\n    \n    def __init__(self, root_path, samples, classes, transform=None, transform_args=[]):\n        \"\"\"_summary_\n\n        Args:\n            root_path (_type_): path from where data will be read. Like `ImageFolder`, pass in folder path where folders inside it will be treated as classes.\n            samples (_type_): a numpy array that will have rows that map `(image_path, class_idx)`\n            classes (_type_): a list of all class names\n            transform (_type_, optional): If provided, can be torch transforms or ðŸ¤— image processors. Defaults to None.\n            transform_args (list, optional): Is a list of `(argument, value)`. Defaults to [].\n        \"\"\"\n        \n        self.root_path = root_path\n        self.samples = samples\n        self.transform = transform\n        self.classes = classes\n\n        self.transform_args = {}\n        for key, val in transform_args:\n            self.transform_args[key] = val\n        \n        print(self.transform_args)\n        \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        file_name, target = self.samples[idx]\n        target = int(target)\n        file_path = os.path.join(self.root_path, self.classes[target], file_name)\n        \n        img = io.imread(file_path)\n        \n        if self.transform is not None:\n            img = self.transform(img, **self.transform_args)\n        \n        return img[\"pixel_values\"][0], target","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T13:49:12.195681Z","iopub.execute_input":"2024-12-09T13:49:12.195973Z","iopub.status.idle":"2024-12-09T13:49:12.203590Z","shell.execute_reply.started":"2024-12-09T13:49:12.195949Z","shell.execute_reply":"2024-12-09T13:49:12.202872Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transform_args = [(\"return_tensors\", 'pt')]\n\ntrain_dataset = CustomDataset(\n    train_path, train_samples, all_classes, transform=image_processor, transform_args=transform_args)\nvalid_dataset = CustomDataset(train_path, test_samples, all_classes,\n                              transform=image_processor, transform_args=transform_args)\nlen(train_dataset), len(valid_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T13:49:12.204676Z","iopub.execute_input":"2024-12-09T13:49:12.204969Z","iopub.status.idle":"2024-12-09T13:49:12.220982Z","shell.execute_reply.started":"2024-12-09T13:49:12.204942Z","shell.execute_reply":"2024-12-09T13:49:12.220276Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset[0][0].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T13:49:12.222101Z","iopub.execute_input":"2024-12-09T13:49:12.222998Z","iopub.status.idle":"2024-12-09T13:49:12.233887Z","shell.execute_reply.started":"2024-12-09T13:49:12.222957Z","shell.execute_reply":"2024-12-09T13:49:12.233056Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def denorm(tensor, mean=mean, std=std):\n  \"\"\"This denormalises the tensor so that we can use it to plot normalised images\"\"\"\n  mean, std = torch.tensor([mean]), torch.tensor([std])\n  # print(mean.shape)\n  output = std * tensor + mean\n  return torch.clamp(output, 0, 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T13:49:12.234914Z","iopub.execute_input":"2024-12-09T13:49:12.235163Z","iopub.status.idle":"2024-12-09T13:49:12.239329Z","shell.execute_reply.started":"2024-12-09T13:49:12.235140Z","shell.execute_reply":"2024-12-09T13:49:12.238526Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_image(tensor, label=None, denormalise=True):\n  \"\"\"tensor is of shape `(h, w, c)`\"\"\"\n  plt.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n\n  if label is not None:\n    plt.title(str(label))\n  \n  if denormalise:\n    tensor = denorm(tensor)\n    tensor.permute(1, 2, 0)\n    \n  plt.imshow(tensor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T13:49:12.240317Z","iopub.execute_input":"2024-12-09T13:49:12.240645Z","iopub.status.idle":"2024-12-09T13:49:12.257108Z","shell.execute_reply.started":"2024-12-09T13:49:12.240610Z","shell.execute_reply":"2024-12-09T13:49:12.256378Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_grid(tensor, n_row):\n  \"\"\"Input is of shape (batch_size, c, h, w)\"\"\"\n\n  grid_tensor = vutils.make_grid(tensor, n_row).permute(1, 2, 0)\n  plot_image(grid_tensor, \"Grid of Random ISL Images\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T13:49:12.258170Z","iopub.execute_input":"2024-12-09T13:49:12.258404Z","iopub.status.idle":"2024-12-09T13:49:12.266386Z","shell.execute_reply.started":"2024-12-09T13:49:12.258381Z","shell.execute_reply":"2024-12-09T13:49:12.265545Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import BatchSampler, SequentialSampler\n\nbatch_size = 64\n\ndef get_loader(dataset, batch_size, num_workers=0):\n    \"\"\"Simple Utility function to instantiate a `DataLoader`.\"\"\"\n    \n    persistent_workers = True if num_workers > 0 else False\n    \n    sampler = BatchSampler(SequentialSampler(dataset), batch_size, drop_last=False)\n    return tdata.DataLoader(dataset, batch_sampler=sampler, \n                            num_workers=num_workers, \n                            # the below flag is necessary for it to not crash\n                            persistent_workers=persistent_workers)\n\ntrain_loader = get_loader(train_dataset, batch_size, 2)\nvalid_loader = get_loader(valid_dataset, batch_size, 2)\n\n# check if the dataloader works fine by plotting a grid of images\nfor images, labels in train_loader:\n    print(images.shape)\n    plot_grid(images, 8)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T13:49:12.267280Z","iopub.execute_input":"2024-12-09T13:49:12.267542Z","iopub.status.idle":"2024-12-09T13:49:14.000145Z","shell.execute_reply.started":"2024-12-09T13:49:12.267519Z","shell.execute_reply":"2024-12-09T13:49:13.998939Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ResNetTrainingModule(pl.LightningModule):\n\n  def __init__(self, num_labels: int, hugging_model_name: str, lr: int = 1e-4):\n    super(ResNetTrainingModule, self).__init__()\n    self.resnet = ResNetForImageClassification.from_pretrained(hugging_model_name, num_labels=num_labels,\n                                                         id2label=idx_to_class, label2id=classes_to_idx,\n                                                         ignore_mismatched_sizes=True)\n    self.lr = lr\n\n  def forward(self, pixel_values):\n    outputs = self.resnet(pixel_values)\n    return outputs[\"logits\"]\n\n  def common_step(self, batch):\n    images, labels = batch\n    outputs = self.resnet(images)[\"logits\"]\n\n    l = F.cross_entropy(outputs, labels)\n    preds = torch.argmax(outputs, dim=-1)\n    num_correct = (preds == labels).sum().item()\n    accuracy = num_correct / len(batch)\n\n    return l, accuracy\n\n  def training_step(self, batch, batch_idx):\n\n    l, accuracy = self.common_step(batch)\n\n    self.log(\"training_loss\", l)\n    self.log(\"training_accuracy\", accuracy)\n\n    return l\n\n  def validation_step(self, batch, batch_idx):\n    l, accuracy = self.common_step(batch)\n\n    self.log(\"val_loss\", l)\n    self.log(\"val_accuracy\", accuracy)\n\n    return l\n\n  def configure_optimizers(self):\n    optimizer = torch.optim.AdamW(self.parameters(), self.lr)\n    return optimizer\n\n  def train_dataloader(self):\n    return train_loader\n\n  def val_dataloader(self):\n    return valid_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T13:49:14.002297Z","iopub.execute_input":"2024-12-09T13:49:14.003372Z","iopub.status.idle":"2024-12-09T13:49:14.017829Z","shell.execute_reply.started":"2024-12-09T13:49:14.003321Z","shell.execute_reply":"2024-12-09T13:49:14.016602Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define paths\ncustom_checkpoint_path = os.path.join(kaggle_write_path, 'custom.pth')\ncheckpoint_path = os.path.join(kaggle_write_path, \"checkpoints\")\n\n# Define callbacks\ncheckpoint_callback = pl.callbacks.ModelCheckpoint(\n    dirpath=checkpoint_path,\n    save_top_k=2,  # Save top 2 models with the lowest validation loss\n    every_n_train_steps=50,  # Save every 50 steps\n    mode=\"min\",  # Minimize the monitored metric\n    monitor=\"val_loss\",  # Monitor validation loss\n    filename=\"{epoch:02d}-{val_loss:.2f}\",  # Filename format\n    verbose=True  # Log saving checkpoints\n)\n\nearly_stop_callback = pl.callbacks.EarlyStopping(\n    monitor='val_loss',  # Monitor validation loss\n    patience=5,          # Stop after 3 epochs with no improvement\n    verbose=True,        # Log when stopping\n    mode='min'           # Minimize validation loss\n)\n\n# Define model\n# num_classes = 35  # Replace with your number of classes\nnum_classes = 23  # Replace with your number of classes\nhugging_model_name = \"microsoft/resnet-50\"\n\nmodel = ResNetTrainingModule(num_classes, hugging_model_name)\n\n# Trainer arguments\nnum_devices = 1  # Set the number of GPUs\ntrainer_args = {\n    \"accelerator\": \"gpu\",\n    \"devices\": num_devices,\n    \"strategy\": \"auto\",\n    \"log_every_n_steps\": 5,\n    \"callbacks\": [early_stop_callback, checkpoint_callback],\n    \"max_epochs\": 30,\n}\n\n# Initialize Trainer\ntrainer = pl.Trainer(**trainer_args)\n\n# Start training\n# trainer.fit(model, ckpt_path=None)\ntrainer.fit(model, ckpt_path=None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T13:49:14.019204Z","iopub.execute_input":"2024-12-09T13:49:14.019548Z","iopub.status.idle":"2024-12-09T13:51:03.996454Z","shell.execute_reply.started":"2024-12-09T13:49:14.019511Z","shell.execute_reply":"2024-12-09T13:51:03.995699Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# Define the mapping for class names (1-9 and A-Z)\n# classes_to_idx = {str(i): i - 1 for i in range(1, 10)}  # Map '1'-'9' to indices 0-8\n# classes_to_idx.update({chr(i): i - ord('A') + 9 for i in range(ord('A'), ord('Z') + 1)})  # Map 'A'-'Z' to indices 9-34\n\ntest_path = '/kaggle/train-test-split/test'\n\nsamples = []\nvisual_samples = []\n# Iterate through subfolders\nfor class_name in sorted(os.listdir(test_path)):\n    class_folder = os.path.join(test_path, class_name)\n    if os.path.isdir(class_folder):  # Ensure it's a directory\n        idx = classes_to_idx[class_name]  # Map class name to index\n        vis_taken = 0\n        # Iterate through images in the subfolder\n        for f in sorted(os.listdir(class_folder)):\n            p = os.path.join(class_folder, f)\n            if vis_taken == 0:\n                visual_samples.append((p, idx))\n                vis_taken = 1\n            samples.append((p, idx))\n\nprint(f\"Total samples: {len(samples)}\")\nprint(f\"Visual samples: {len(visual_samples)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T13:51:03.997742Z","iopub.execute_input":"2024-12-09T13:51:03.998166Z","iopub.status.idle":"2024-12-09T13:51:04.007213Z","shell.execute_reply.started":"2024-12-09T13:51:03.998138Z","shell.execute_reply":"2024-12-09T13:51:04.006534Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def perform_predictions(samples, model):\n    \"\"\"samples will be of the form `(path, target)`\"\"\"\n    num_correct = 0\n    \n    fig, ax = plt.subplots(len(samples), sharey=True, figsize=(5, 50))\n    model.eval()\n    for curr_ax, (p, idx) in zip(ax, samples):\n        img = io.imread(p)\n        proc_img = image_processor(img, return_tensors=\"pt\")[\"pixel_values\"]\n        outputs = model(proc_img)\n        \n        pred = torch.argmax(outputs, -1)\n        if pred == idx:\n            num_correct += 1\n        \n        target_class = all_classes[idx]\n        curr_ax.title.set_text(f\"Actual class: {target_class}, Predicted: {all_classes[pred]}\")\n        curr_ax.tick_params(bottom=False, left=False, labelleft=False,\n                            labelright=False, labelbottom=False)\n        curr_ax.imshow(img)\n        \n    plt.show()\n    return num_correct\n    \nperform_predictions(visual_samples, model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T13:51:04.008184Z","iopub.execute_input":"2024-12-09T13:51:04.008422Z","iopub.status.idle":"2024-12-09T13:51:09.606559Z","shell.execute_reply.started":"2024-12-09T13:51:04.008398Z","shell.execute_reply":"2024-12-09T13:51:09.605835Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport torch\nfrom tqdm import tqdm  # Optional, for progress display\nfrom PIL import Image\n\ndef evaluate_model(samples, model, image_processor):\n    \"\"\"samples will be of the form `(path, target)`\"\"\"\n    all_preds = []\n    all_labels = []\n    \n    fig, ax = plt.subplots(len(samples), sharey=True, figsize=(5, 50))\n    model.eval()\n    for curr_ax, (p, idx) in zip(ax, samples):\n        img = io.imread(p)\n        proc_img = image_processor(img, return_tensors=\"pt\")[\"pixel_values\"]\n        outputs = model(proc_img)\n        \n        pred = torch.argmax(outputs, -1)\n        all_preds.append(pred)\n        all_labels.append(idx)\n\n    # Calculate accuracy, precision, recall, and F1 score\n    accuracy = accuracy_score(all_labels, all_preds)\n    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n    \n    # Print the results\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1 Score: {f1:.4f}\")\n    \n    return accuracy, precision, recall, f1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T13:51:09.607584Z","iopub.execute_input":"2024-12-09T13:51:09.607816Z","iopub.status.idle":"2024-12-09T13:51:09.615041Z","shell.execute_reply.started":"2024-12-09T13:51:09.607794Z","shell.execute_reply":"2024-12-09T13:51:09.614159Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"evaluate_model(samples, model, image_processor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T13:51:09.619316Z","iopub.execute_input":"2024-12-09T13:51:09.619904Z","iopub.status.idle":"2024-12-09T13:51:40.706699Z","shell.execute_reply.started":"2024-12-09T13:51:09.619867Z","shell.execute_reply":"2024-12-09T13:51:40.705793Z"}},"outputs":[],"execution_count":null}]}